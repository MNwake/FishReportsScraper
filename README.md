# Fish Survey Scraper

The Fish Survey Scraper is a Python-based web scraper built with Playwright that extracts fish survey data from the Minnesota Department of Natural Resources (MN DNR) website along with additional supplementary data from other sources. This repository is part of the larger Fish Surveys project and is designed to feed clean and validated data to the server component for further processing.

## Features

### Web Scraping

- Utilizes Playwright to efficiently navigate and extract fish survey data from MN DNR and other relevant websites.

### Data Validation

- A built-in Tkinter application provides a full spreadsheet view with search functionality, allowing users to verify that the scraped data is accurate and complete.

### Data Management

- Scraped data is saved in a structured directory, making it easy for the server to parse and integrate into the overall system.

### Modern Python Tools

Leverages powerful libraries including:

- Playwright: For robust, asynchronous web scraping
- Pydantic: For data validation and settings management
- Tkinter: For the graphical user interface (GUI) to control the scraper and view results
- Matplotlib: For any potential data visualization within the validation UI

## Installation

### Prerequisites

- Python 3.8+
  - Ensure that Python is installed on your system. You can download it from the official website.

```bash
git clone https://github.com/MNwake/FishReportsScraper.git
cd FishReportsScraper
```

### Setup Virtual Environment

It is recommended to use a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
```

### Install Dependencies

Install the required Python packages using pip:

```bash
pip install -r requirements.txt
```

After installing Playwright, install the necessary browsers:

```bash
playwright install
```

## Usage

### Running the Scraper

The scraper is integrated into a Tkinter-based GUI application. To start the application:

```bash
python main.py
```

Upon launching, the Tkinter application will display:

- Control Panel: Start and stop buttons to manage the scraping process
- Data Spreadsheet: A searchable table for reviewing scraped data, ensuring that all information has been captured correctly
- Visualizations: (Optional) Any graphs generated by Matplotlib to provide insights into the data

### Workflow Overview

1. Start Scraping:
   - Click the "Start" button to initiate the scraping process. The scraper will begin retrieving fish survey data from MN DNR and other supplementary sites.
2. Data Storage:
   - Scraped data is automatically saved in a designated directory. The data structure is organized to facilitate easy parsing by the server.
3. Validation:
   - Use the embedded spreadsheet tool to search and verify the accuracy of the scraped data.
4. Stop Scraping:
   - The "Stop" button will gracefully terminate the scraping process if needed.

## Configuration

Configuration settings (e.g., target URLs, output directories, and scraping intervals) are managed via Pydantic models. These settings can be adjusted in the configuration file or directly within the code to suit your project requirements.

## Contributing

Contributions are welcome! If you have suggestions, bug fixes, or improvements:

1. Fork the repository
2. Create a new branch for your feature or fix
3. Submit a pull request detailing your changes

## License

This project is licensed under the MIT License. See the LICENSE file for details.

## Acknowledgments

- MN DNR: For providing the data that powers this project
- Playwright: For an excellent tool for web scraping and automation
- Tkinter & Matplotlib: For enabling rapid development of user interfaces and visualizations in Python
- Pydantic: For making data validation straightforward and efficient

For any issues or feature requests, please open an issue on the GitHub repository.

Happy scraping!
